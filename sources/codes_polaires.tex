\begin{comment}
\end{comment}

\chapter{Étude des codes polaires convolutifs}

Le problème de communication classique est défini comme suit.
Deux acteurs, 
qui se nomment Arthur et Béatrice pour faire changement\footnote{
  Une recherche en ligne vous démontrera que beaucoup d'attention a été donnée 
  à Alice et Bob. 
},
veulent échanger de l'information.
Dans ce cas,
supposons qu'Arthur cherchent à envoyer un message à Béatrice qui se trouve
des kilomètres plus loin.
Pour ce faire,
Arthur utilise son téléphone cellulaire et envoie un texto à Béatrice.
Une fois le message composé,
celui-ci est converti en une séquence de nombres binaires (des 0 et des 1)
qui sera transmise via des ondes radios jusqu'au téléphone de Béatrice
qui devra reconvertir la séquence de 0 et de 1 en message intelligible.

Cependant, 
l'histoire ne s'arrête pas là puisque lors de la transmission du signal
dans l'atmosphère, il est fort probable que celui-ci soit corrompu en 
raison d'une interaction indésirée.
La solution à ce problème est de transmettre une séquence de nombres binaires
plus longue que ce qui est nécessaire en espérant que l'information supplémentaire
  nous aide à retrouver le message initial en cas d'erreur.

Dans ce chapitre,
je vais d'abord reformuler ce problème dans un langage mathématique plus formel.  
Par la suite, 
je vais présenter les réseaux de tenseurs,
un outil mathématique qui va m'être très utile pour décrire les codes polaires 
ainsi qu'une généralisation de ces derniers, les codes polaires convolutifs.
Ces codes sont très importants puisque les codes polaires sont au coeur 
de la technologie de communication 5G et que toutes améliorations de ceux-ci
a un grand potentiel d'application.
À cet effet, le chapitre se termine par une présentation d'un article scientifique
dans lequel j'identifie les régimes où les performances des codes polaires convolutifs sont 
les plus impressionnantes.

\section{Communication classique et correction d'erreurs}

L'unité fondamentale de l'information est le bit.
Un bit prend la value 0 ou 1 et correspond à la quantité 
d'information acquiérie après avoir appris la réponse 
à une question ayant deux choix de réponses (oui et non par exemple).
Formellement, un bit est un élément du corps fini de cardinalité 2 noté $\bit = \qty{0, 1}$.
La multiplication dans $\bit$ est définie comme à l'habitude
et l'addition est définie module 2, soit que $1 + 1 = 0$.

Je m'intéresse au cas où Arthur veut transmettre une séquence 
de $k$ bits $\vb{x} \in \bit^k$ en utilisant un canal bruité $\canal$.
Dans cette thèse,
je vais me limiter à une définiton simple d'un canal bruité,
bien qu'il soit possible de généraliser cette notion. 
Le canal bruité que je considère est le canal symmétrique binaire.
Celui-ci est une fonction probabiliste $\mathcal \canal_p: \bit \to \bit$
qui, après avoir reçu le bit $b$ comme entrée, 
retourne le même bit $b$ avec probabilité $1 - p$ et retourne le
bit opposé $1 + b$ avec probabilité $p$.
Il est commun de nommer $p$ la probabilité de renversement 
ou la probabilité d'erreur.

Dans le cas où $k > 1$ bits sont transmis via un canal bruité $\canal_p$,
un canal effectif $\canal_p^k$ composé de $k$ copies de $\canal_p$
est considéré.
Par exemple,
si le message $00000$ est transmis,
le message $01010$ est reçu avec probabilité $p^2(1 - p)^3$ 
tandis que le message $11111$ est reçu avec probabilité $p^5$.
De façon générale, 
la probabilité que le message original soit reçu sans erreur est de $(1 - p)^k$.
Cette probabilité décroit exponentiellement avec la taille du message 
et il est rapidement improbable que cela se produise.

Pour contrer ce problème,
chaque message original $\vb x \in \bit^k$ est encodé dans une autre séquence 
unique de bits $\vb{y} \in \bit^n$ telle que $n > k$.
Comme le nombre de messages dans $\bit^k$ est inférieur au nombre
de séquences dans $\bit^n$,
seul un sous-espace $\code \subset \bit^n$ est utilisé.
Ce sous-espace $C$ définit un code correcteur d'erreurs
de $k$ bits logiques et de $n$ bits physiques 
et un élément de $C$ est nommé mot-code.

\begin{table}[t]
  \caption{Exemple d'encodage de 2 bits vers 6 bits}
  \label{tab:exemple_encodage}
  \begin{center}
    \begin{tabular}[c]{cc}
      \textbf{Message} & \textbf{Mot-code} \\
      \hline
      00 & 000000 \\
      01 & 101010 \\
      10 & 010101 \\
      11 & 111111
    \end{tabular}
  \end{center}
\end{table}

Le Tableau~\ref{tab:exemple_encodage} illustre un exemple de code correcteur
encodant 2 bits logiques à l'aide de 6 bits physiques.
En utilisant ce code correcteur,
Béatrice est en mesure de retrouver le message initial qu'Arthur lui a
envoyé s'il y a au plus une erreur qui affecte les 6 bits transmis. 
Par exemple, si le message 01 est transmis à l'aide du mot-code 
101010 et que la séquence 111010 est reçu, 
il est aisé, en comparant cette séquence avec les différents 
mots-codes, d'identifier le message envoyé. 
Par contre, 
si deux erreurs affectait le mot-code générant la séquence 111110, 
Béatrice concluerait à tord que le message 11 a été envoyé par Arthur.
L'opération qui consiste à essayer de retrouver le message original à partir
de la séquence reçue se nomme décodage 
et une erreur logique est une erreur à la suite du décodage.

Pour cet exemple,
la probabilité de transmettre un message de 2 bits sans encodage avec succès 
est de $(1 - p)^2$. 
En comparaison,
lorsque l'encodage de 6 bits est utilisé, 
la probabilité que Béatrice soit en mesure de retrouver le message original 
sans erreur logique est de $(1 - p)^6 + 6p(1 - p)^5$.
Ainsi, si $p \lesssim 0.22$, il est avantageux d'utiliser le code correcteur.

Le défi de la correction d'erreur est de construire des codes correcteurs 
qui réduisent considérablement la probabilité d'une erreur logique sans utiliser 
un trop grand nombre de bits supplémentaires.
À cet effet, c'est en 1948 que Claude Shannon a démontré la plus grande valeur 
de rendement $R = k/n$ qui peut être transmise 
via un canal bruité~\cite{shannon_mathematical_1948}.
Plus précisement,
la capacité $\capacite$ d'un canal bruité est le rendement maximum 
que peuvent transmettre plusieurs copies du canal tel que la probabilité d'une 
erreur logique tend vers 0 lorsque $n \to \infty$.
Autrement dit,
pour un canal de capacité $\capacite$, 
il existe une famille de codes $\qty{C_i}_{i\in\mathbb N}$ avec 
$\max_{i} k_i / n_i = \capacite$ telle que la probabilité d'une erreur de 
décodage décroit exponentiellement avec $i$.

Pour le canal symmétrique binaire $\canal_p$, 
il est prouvé~\cite{shannon_mathematical_1948} que la capacité est 
\begin{equation}
  \capacite(\canal_p) = 1 - H_2(p)
\end{equation}
avec l'entropie binaire $H_2(p) = -p \log(p) - (1 - p)\log(1 - p)$~\footnote{
  Pour alléger le texte, je n'inclus pas les preuves qui se retrouvent facilement 
  dans la littérature.
  De plus, les concepts importants de la théorie de l'information, dont l'entropie, 
  sont présentés à l'Annexe~\ref{chap:theo_info}.
}. 
La Figure~\ref{fig:capacite_canal} illustre que la capacité du canal symmétrique binaire 
est nulle à $p = 0.5$.
En effet, un canal ayant une probabilité d'erreur de $0.5$ retourne 
chacune des séquences de bits avec la même probabilité 
et cela, peu importe le message transmis.
Un tel canal est donc inutilisable.
En contrepartie, la capacité est maximale à $p = 0$ et $p = 1$.
Lorsque la probabilité d'erreur est nulle,
il n'y a aucun avantage à utiliser un code correcteur 
et il suffit de transmettre le message directement.
Par contre, il est un peu plus surprenant que la capacité soit maximale lorsque la probabilité
d'erreur est de 1.
Cependant, en utilisant un tel canal, 
il suffit de renverser chacun des bits reçus pour retrouver le message transmis sans erreur.
De même, 
un canal avec une probabilité d'erreur $1 - p$ est équivalent à un canal de probabilité d'erreur $p$
après avoir renversés tous les bits reçus.
C'est pourquoi, 
les études numériques de cette thèse se concentre sur des plages de probabilités entre 0 et 0.5.

\begin{figure}
  \begin{center}
    \includegraphics{figures/capacite_canal.pdf}
  \end{center}
  \caption{Capacité du canal binaire symmétrique selon la probabilité d'erreur}
  \label{fig:capacite_canal}
\end{figure}

Il est important de noté que l'article de Shannon n'inclut aucune construction de codes 
correcteurs permettant d'atteindre la capacité du canal binaire symmétrique.
En effet, les codes polaires sont les premiers codes correcteurs découverts 
pour lesquels il existe un algorithme de décodage efficace permettant d'atteindre 
la capacité d'un canal~\cite{arikan_channel_2009}. 
En ce sens, il s'agit d'une famille de codes qui optimise le compromis en réduction du bruit 
et rendement.

Avant d'introduire les codes polaires et les codes polaires convolutifs,
je vais faire un petit détour à la prochaine section pour introduire les réseaux de tenseurs.
Cet outil mathématique me permettera alors d'exprimer de façon plus concise la construction
de ces codes et les algorithmes de décodage correspondant.

\section{Réseaux de tenseurs}
\label{sec:reseaux_tenseurs}

Il existe plusieurs articles d'introductions aux réseaux de tenseurs CITATIONS RÉSEAUX DE TENSEURS
qui couvre plusieurs applications de cet outil principalement en physique du solide,
mais également pour des domaines comme l'informatique quantique et l'apprentissage automatique. 
Pour ma part, 
je vais me limiter à introduire les notions essentielles à la construction des 
codes polaires et des codes polaires convolutifs.
Pour cela, 
je vais considéré les réseaux de tenseurs comme un outil graphique pour représenter 
des problèmes d'algèbre linéaire et je ne discuterai pas des considérations plus physiques 
(corrélations, entropie, etc) qui accompagnent généralement une introduction du sujet.
J'invite toute personne intéressée d'en apprendre plus sur le sujet à consulter les ouvrages 
cités précédemment.

Intuitivement, 
un tenseur est la généralisation d'une matrice.
En effet,
tout comme une matrice, 
un tenseur peut être compris comme une boite de nombres identifier par une liste d'indices. 
Pour les matrices,
il suffit d'une paire d'indices pour identifier un élément,
alors qu'un nombre arbitraire d'indices peuvent être utiliser pour un tenseur. 
Le rang d'un tenseur est le nombre d'indices utilisé pour identifier un élément.
Par exemple, 
un scalaire est un tenseur de rang 0, 
un vecteur est un tenseur de rang 1 
et une matrice est un tenseur de rang 2.

Une des opérations parmi les plus importantes sur les tenseurs est la contraction.
La contraction de deux tenseurs est la généralisation du produit matricielle.
Avant de définir la contraction,
il est intéressant de revenir un peu sur le produit matricielle.

Pour une matrice $A$ de dimension $l \times m$ et une matrice $B$ de dimension $m \times n$,
les éléments du produit matricielle $C = AB$ sont donnés par
\begin{equation}
  C_{ij} = \sum_{k=0}^{m - 1} A_{ik} B_{kj}.
\end{equation}
Chacun des éléments de $C$ est obtenu en faisant 
la somme sur le second indice de $A$ et le premier de $B$ 
en fixant la valeur des indices restant.
Le dernier indice de $A$ et le premier indice de $B$ forment 
alors un indice common de la paire $(A, B)$.
La contraction de deux tenseurs de rang arbitraire est définie 
de la même façon.
Par exemple, 
le tenseur $C$ dont les éléments sont
\begin{equation}
  C_{hijkl} = \sum_{r} A_{hrij} B_{klr}
\end{equation}
est la contraction du tenseur $A$, de rang 4,
et du tenseur $B$, de rang 3, 
ayant comme indice commun,
le deuxième indice de $A$ et le troisième de $B$\footnote{
En pratique, pour calculer la contraction de deux tenseurs, 
une série d'opérations de remodelage et de permutation des indices 
est effectuée pour représenter les tenseurs comme des matrices et 
ainsi profiter des nombreuses optimisations développées pour les algorithmes
de multiplication matricielle.
Je considère cependant qu'il s'agit d'un détail d'implémentation 
et je ne m'attenderai pas sur ce point.}.
Une contraction n'est pas limitée à un seul indice commun.
En effet,
pour les mêmes tenseurs $A$ et $B$ le tenseur $D$ ayant les éléments
\begin{equation}
  D_{hik} = \sum_{r} \sum_{s} A_{hris} B_{ksr}
  \label{eq:contraction_somme}
\end{equation}
est une contraction de deux indices de $A$ et $B$.

\begin{figure}[t]
  \begin{center}
    \includegraphics{figures/contraction_reseau}
  \end{center}
  \caption{
    La contraction d'un réseau de deux tenseurs 
    correspondant à l'Équation~\eqref{eq:contraction_somme}.
  }
  \label{fig:contraction_reseau}
\end{figure}

Dans les Sections~\ref{sec:codes_polaires} et~\ref{sec:codes_polaires_conv},
j'utiliserai plusieurs dizaines voir centaines de tenseurs pour 
représenter des distributions de probabilité.
Dans ce cas, 
la notation de l'Équation~\eqref{eq:contraction_somme} devient inutilisable 
en raison du nombre trop important de tenseurs et d'indices.
Pour contourner ce problème,
j'utiliserai une notation graphique pour les tenseurs.
C'est cette représentation graphique qui porte le nom de réseaux de tenseurs.

Un réseau de tenseur est défini par un graphe $G = (\tenseurs, \aretes)$ où $\tenseurs$ 
est un ensemble de tenseurs et $\aretes \subseteq (\tenseurs \times \tenseurs) \cup \tilde{\tenseurs}$ 
est un ensemble d'arêtes fermées, $\tenseurs \times \tenseurs$, reliant deux tenseurs 
et d'arêtes ouvertes, $\tilde \tenseurs = \qty{\qty{T} : T \in \tenseurs}$, comprenant un seul tenseur.
Une arête $a \in \aretes$ est connectée à un tenseur si $T \in \tenseurs$ si $T \in a$.
Le rang d'un tenseur $T \in \tenseurs$ correspond au degré de $T$ dans $G$,
soit le nombre d'arêtes connectées à $T$.
Ainsi, chaque arête connectée à $T$ correspond à un indice de $T$.

Une arête fermée correspond à un indice commun entre deux tenseurs à contracter.
Suite à la contraction d'un indice commun,
le réseau est modifié.



La Figure~\ref{fig:contraction_reseau} illustre la contraction d'un réseau de deux tenseurs





\section{Codes polaires}
\label{sec:codes_polaires}

\section{Codes polaires convolutifs}
\label{sec:codes_polaires_conv}

\section{Article : Comparaison de la profondeur et la largeur des codes polaires convolutifs}
