\begin{comment}
\end{comment}

\chapter{Étude des codes polaires convolutifs}

Le problème de communication classique est défini comme suit.
Deux acteurs, 
qui se nomment Arthur et Béatrice pour faire changement\footnote{
  Une recherche en ligne vous démontrera que beaucoup d'attention a été donnée 
  à Alice et Bob. 
},
veulent échanger de l'information.
Dans ce cas,
supposons qu'Arthur cherchent à envoyer un message à Béatrice qui se trouve
des kilomètres plus loin.
Pour ce faire,
Arthur utilise son téléphone cellulaire et envoie un texto à Béatrice.
Une fois le message composé,
celui-ci est converti en une séquence de nombres binaires (des 0 et des 1)
qui sera transmise via des ondes radios jusqu'au téléphone de Béatrice
qui devra reconvertir la séquence de 0 et de 1 en message intelligible.

Cependant, 
l'histoire ne s'arrête pas là puisque lors de la transmission du signal
dans l'atmosphère, il est fort probable que celui-ci soit corrompu en 
raison d'une interaction indésirée.
La solution à ce problème est de transmettre une séquence de nombres binaires
plus longue que ce qui est nécessaire en espérant que l'information supplémentaire
  nous aide à retrouver le message initial en cas d'erreur.

Dans ce chapitre,
je vais d'abord reformuler ce problème dans un langage mathématique plus formel.  
Par la suite, 
je vais présenter les réseaux de tenseurs,
un outil mathématique qui va m'être très utile pour décrire les codes polaires 
ainsi qu'une généralisation de ces derniers, les codes polaires convolutifs.
Ces codes sont très importants puisque les codes polaires sont au coeur 
de la technologie de communication 5G et que toutes améliorations de ceux-ci
a un grand potentiel d'application.
À cet effet, le chapitre se termine par une présentation d'un article scientifique
dans lequel j'identifie les régimes où les performances des codes polaires convolutifs sont 
les plus impressionnantes.

\section{Communication classique et correction d'erreurs}

L'unité fondamentale de l'information est le bit.
Un bit prend la value 0 ou 1 et correspond à la quantité 
d'information acquiérie après avoir appris la réponse 
à une question ayant deux choix de réponses (oui et non par exemple).
Formellement, un bit est un élément du corps fini de cardinalité 2 noté $\bit = \qty{0, 1}$.
La multiplication dans $\bit$ est définie comme à l'habitude
et l'addition est définie module 2, soit que $1 + 1 = 0$.

Je m'intéresse au cas où Arthur veut transmettre une séquence 
de $k$ bits $\vb{x} \in \bit^k$ en utilisant un canal bruité $\canal$.
Dans cette thèse,
je vais me limiter à une définiton simple d'un canal bruité,
bien qu'il soit possible de généraliser cette notion. 
Le canal bruité que je considère est le canal symmétrique binaire.
Celui-ci est une fonction probabiliste $\mathcal \canal_p: \bit \to \bit$
qui, après avoir reçu le bit $b$ comme entrée, 
retourne le même bit $b$ avec probabilité $1 - p$ et retourne le
bit opposé $1 + b$ avec probabilité $p$.
Il est commun de nommer $p$ la probabilité de renversement 
ou la probabilité d'erreur.

Dans le cas où $k > 1$ bits sont transmis via un canal bruité $\canal_p$,
un canal effectif $\canal_p^k$ composé de $k$ copies de $\canal_p$
est considéré.
Par exemple,
si le message $00000$ est transmis,
le message $01010$ est reçu avec probabilité $p^2(1 - p)^3$ 
tandis que le message $11111$ est reçu avec probabilité $p^5$.
De façon générale, 
la probabilité que le message original soit reçu sans erreur est de $(1 - p)^k$.
Cette probabilité décroit exponentiellement avec la taille du message 
et il est rapidement improbable que cela se produise.

Pour contrer ce problème,
chaque message original $\vb x \in \bit^k$ est encodé dans une autre séquence 
unique de bits $\vb{y} \in \bit^n$ telle que $n > k$.
Comme le nombre de messages dans $\bit^k$ est inférieur au nombre
de séquences dans $\bit^n$,
seul un sous-espace $\code \subset \bit^n$ est utilisé.
Ce sous-espace $C$ définit un code correcteur d'erreurs
de $k$ bits logiques et de $n$ bits physiques 
et un élément de $C$ est nommé mot-code.

\begin{table}[t]
  \caption{Exemple d'encodage de 2 bits vers 6 bits}
  \label{tab:exemple_encodage}
  \begin{center}
    \begin{tabular}[c]{cc}
      \textbf{Message} & \textbf{Mot-code} \\
      \hline
      00 & 000000 \\
      01 & 101010 \\
      10 & 010101 \\
      11 & 111111
    \end{tabular}
  \end{center}
\end{table}

Le Tableau~\ref{tab:exemple_encodage} illustre un exemple de code correcteur
encodant 2 bits logiques à l'aide de 6 bits physiques.
En utilisant ce code correcteur,
Béatrice est en mesure de retrouver le message initial qu'Arthur lui a
envoyé s'il y a au plus une erreur qui affecte les 6 bits transmis. 
Par exemple, si le message 01 est transmis à l'aide du mot-code 
101010 et que la séquence 111010 est reçu, 
il est aisé, en comparant cette séquence avec les différents 
mots-codes, d'identifier le message envoyé. 
Par contre, 
si deux erreurs affectait le mot-code générant la séquence 111110, 
Béatrice concluerait à tord que le message 11 a été envoyé par Arthur.
L'opération qui consiste à essayer de retrouver le message original à partir
de la séquence reçue se nomme décodage 
et une erreur logique est une erreur à la suite du décodage.

Pour cet exemple,
la probabilité de transmettre un message de 2 bits sans encodage avec succès 
est de $(1 - p)^2$. 
En comparaison,
lorsque l'encodage de 6 bits est utilisé, 
la probabilité que Béatrice soit en mesure de retrouver le message original 
sans erreur logique est de $(1 - p)^6 + 6p(1 - p)^5$.
Ainsi, si $p \lesssim 0.22$, il est avantageux d'utiliser le code correcteur.

Le défi de la correction d'erreur est de construire des codes correcteurs 
qui réduisent considérablement la probabilité d'une erreur logique sans utiliser 
un trop grand nombre de bits supplémentaires.
À cet effet, c'est en 1948 que Claude Shannon a démontré la plus grande valeur 
de rendement $R = k/n$ qui peut être transmise 
via un canal bruité~\cite{shannon_mathematical_1948}.
Plus précisement,
la capacité $\capacite$ d'un canal bruité est le rendement maximum 
que peuvent transmettre plusieurs copies du canal tel que la probabilité d'une 
erreur logique tend vers 0 lorsque $n \to \infty$.
Autrement dit,
pour un canal de capacité $\capacite$, 
il existe une famille de codes $\qty{C_i}_{i\in\mathbb N}$ avec 
$\max_{i} k_i / n_i = \capacite$ telle que la probabilité d'une erreur de 
décodage décroit exponentiellement avec $i$.

Pour le canal symmétrique binaire $\canal_p$, 
il est prouvé~\cite{shannon_mathematical_1948} que la capacité est 
\begin{equation}
  \capacite(\canal_p) = 1 - H_2(p)
\end{equation}
avec l'entropie binaire $H_2(p) = -p \log(p) - (1 - p)\log(1 - p)$~\footnote{
  Pour alléger le texte, je n'inclus pas les preuves qui se retrouvent facilement 
  dans la littérature.
  De plus, les concepts importants de la théorie de l'information, dont l'entropie, 
  sont présentés à l'Annexe~\ref{chap:theo_info}.
}. 
La Figure~\ref{fig:capacite_canal} illustre que la capacité du canal symmétrique binaire 
est nulle à $p = 0.5$.
En effet, un canal ayant une probabilité d'erreur de $0.5$ retourne 
chacune des séquences de bits avec la même probabilité 
et cela, peu importe le message transmis.
Un tel canal est donc inutilisable.
En contrepartie, la capacité est maximale à $p = 0$ et $p = 1$.
Lorsque la probabilité d'erreur est nulle,
il n'y a aucun avantage à utiliser un code correcteur 
et il suffit de transmettre le message directement.
Par contre, il est un peu plus surprenant que la capacité soit maximale lorsque la probabilité
d'erreur est de 1.
Cependant, en utilisant un tel canal, 
il suffit de renverser chacun des bits reçus pour retrouver le message transmis sans erreur.
De même, 
un canal avec une probabilité d'erreur $1 - p$ est équivalent à un canal de probabilité d'erreur $p$
après avoir renversés tous les bits reçus.
C'est pourquoi, 
les études numériques de cette thèse se concentre sur des plages de probabilités entre 0 et 0.5.

\begin{figure}
  \begin{center}
    \includegraphics{figures/capacite_canal.pdf}
  \end{center}
  \caption{Capacité du canal binaire symmétrique selon la probabilité d'erreur}
  \label{fig:capacite_canal}
\end{figure}



Il est important de noté que l'article de Shannon n'inclut aucune construction de codes 
correcteurs permettant d'atteindre la capacité du canal binaire symmétrique.
En effet, les codes polaires sont les premiers codes correcteurs découverts 
pour lesquels il existe un algorithme de décodage efficace permettant d'atteindre 
la capacité d'un canal~\cite{arikan_channel_2009}. 
En ce sens, il s'agit d'une famille de codes qui optimise le compromis en réduction du bruit 
et rendement.

Avant d'introduire les codes polaires et les codes polaires convolutifs,
je vais faire un petit détour à la prochaine section pour introduire les réseaux de tenseurs.
Cet outil mathématique me permettera alors d'exprimer de façon plus concise la construction
de ces codes et les algorithmes de décodage correspondant.

\section{Réseaux de tenseurs}




\section{Codes polaires}

\section{Codes polaires convolutifs}

\section{Article : Comparaison de la profondeur et la largeur des codes polaires convolutifs}
